{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging using CRF++\n",
    "After stemming, run the Part-of-Speech tagger using the command below. This will produced a file that contains the sentences written vertiacally with the corresponding stemming and tagging information beside each of the word.\n",
    "\n",
    "Command format:\n",
    "\n",
    "`stemmer.out` - output file of stemming notebook\n",
    "\n",
    "`tagger.out` - output file of pos tagging\n",
    "\n",
    "`crf_test -m path/to/Bigram.model path/to/stemmer.out -o path/to/tagger.out`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "crf_test -m ../pos_tagger/Bigram.model ../files/stemmer.out -o ../files/pos_tagger.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------\n",
    "# Data Transformation\n",
    "- Word-pos pair Extraction\n",
    "- Coarse tag Extraction\n",
    "- Data conversion for the rule-based algorithm consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import math\n",
    "import pandas\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method extracts only the coarse tag given to a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tags(file):\n",
    "    \"\"\"\n",
    "    Transform the input file into an array of tuples.\n",
    "    Each tuple contains the word and its corresponding tag.\n",
    "    \"\"\"\n",
    "\n",
    "    text = file.readlines()\n",
    "    array = []\n",
    "    sentence = []\n",
    "    tuple = ()\n",
    "\n",
    "    for line in text:\n",
    "        data = line.split(\"\\t\")\n",
    "        if data[0] != \"?\" and len(line.strip()) > 0:\n",
    "            word = data[0]\n",
    "            pos = data[len(data) - 1]\n",
    "\n",
    "            # Extract only the coarse tag of the word\n",
    "            pos = get_coarsetag(pos, 1)\n",
    "\n",
    "            tuple = (word, pos.strip())\n",
    "            sentence.append(tuple)\n",
    "        elif data[0] == '?':\n",
    "            array.append(sentence)\n",
    "            sentence = []\n",
    "            tuple = ()\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to the `get_tags` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coarsetag(tag, level):\n",
    "    \"\"\"\n",
    "    Separate the coarse tags from the fine pos tags.\n",
    "    \n",
    "    tag - the complete pos tag\n",
    "    level - desired tag level\n",
    "    \"\"\"\n",
    "\n",
    "    if level == 1:\n",
    "        tag = tag.split(\"-\")[0]\n",
    "    elif level == 2:\n",
    "        temp = tag.split(\"-\")[1]\n",
    "        tag = temp[:2]\n",
    "    elif level == 3:\n",
    "        temp = tag.split(\"-\")[1]\n",
    "        tag = temp[2:]\n",
    "\n",
    "    return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3077"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuples = get_tags(open(os.path.abspath('../files/pos_tagger.out')))\n",
    "len(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_data(file, column_no):\n",
    "    \"\"\"\n",
    "    Get every data in the specified column_no.\n",
    "    \"\"\"\n",
    "\n",
    "    r = csv.reader(file)\n",
    "    data = []\n",
    "    \n",
    "    for row in r:\n",
    "        if column_no == 0:\n",
    "            if row[0] != \"Questions\":\n",
    "                data.append(row[0].strip())\n",
    "        elif column_no == 1:\n",
    "            if row[1] != \"Category\":\n",
    "                data.append(row[1])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3077"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = get_raw_data(open(os.path.abspath('../files/raw_data/labelled_data.csv')), 0)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bakit ka masyadong praning?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('Bakit', 'PR'), ('ka', 'PR'), ('masyadong', 'RB'), ('praning', 'RB')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[85]\n",
    "tuples[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prune(tuples_array):\n",
    "    \"\"\"\n",
    "    Extract the independent clause of compound questions.\n",
    "    Simple questions are left untouch.\n",
    "    \"\"\"\n",
    "\n",
    "    conj = {\"sapagkat\", \"dahil\", \"dahil sa\", \"at saka\", \"at hindi\", \"ni hindi\",\n",
    "            \"pero\", \"datapwat\", \"ngunit\", \"subalit\", \"o\", \"o kaya\",\n",
    "            \"gayon pa man\", \"gayumpaman\", \"gayunman\", \"kaya\", \"kung kaya't\",\n",
    "            \"kung kaya\", \"man\", \"maging\", \"hindi lamang\", \"kundi\", \"bagaman\",\n",
    "            \"bagama't\", \"kapag\", \"kasi\", \"dahilan sa\", \"gawa ng\", \"porke\",\n",
    "            \"porke at\", \"porke't\", \"pagkat\", \"kaya\", \"kaysa\", \"kahit\",\n",
    "            \"gayong\", \"kung\", \"kung gayon\", \"habang\", \"nang\", \"nang sa gayon\",\n",
    "            \"maging\", \"maliban kung\", \"palibhasa\", \"para\", \"upang\", \"parang\",\n",
    "            \"pansamantala\", \"hanggang\"\n",
    "            }\n",
    "\n",
    "    q_words = {'aling', 'alin-alin', 'alin-aling', 'saang', 'saan-saan',\n",
    "               'nasaan', 'nasaang', 'anong', 'anu-ano', 'anu-anong', 'inaano',\n",
    "               'paanong', 'papaano', 'papaanong', 'sinong', 'sinu-sino',\n",
    "               'sino-sinong', 'kailang', 'alin', 'saan', 'ano', 'kailan',\n",
    "               'paano', 'sino', 'bakit'\n",
    "               }\n",
    "    \n",
    "    with_tags = tuples_array\n",
    "    \n",
    "    # Scan data for compound question and extract the independent clause\n",
    "    for i in range(0, len(with_tags)):\n",
    "        text = with_tags[i]\n",
    "        \n",
    "        if len(with_tags[i]) == 0:\n",
    "            print(len(questions))\n",
    "\n",
    "        for pair in with_tags[i]:\n",
    "            if pair[0] in conj:\n",
    "                index = text.index(pair)\n",
    "\n",
    "                # Conjuction should not be too near in the beginning of the\n",
    "                # questions\n",
    "                if len(text) >= 5 and index >= math.ceil(len(text)/2):\n",
    "                    with_tags[i] = tuples_array[i][:index]\n",
    "                elif index == 0:\n",
    "                    for j in range(index, len(text)):\n",
    "                        if text[j][0].lower() in q_words:\n",
    "                            with_tags[i] = tuples_array[i][j:]\n",
    "                            break\n",
    "\n",
    "    # Scan each tuple array for arrays not starting with a wh-word\n",
    "    # Scan questions without conjuctions but does not start with wh-word\n",
    "    for i in range(0, len(with_tags)):\n",
    "        tagged_text = with_tags[i]\n",
    "        \n",
    "        if tagged_text[0][0].lower() not in q_words:\n",
    "\n",
    "            for j in range(0, len(tagged_text)):\n",
    "                if tagged_text[j][0].lower() in q_words:\n",
    "                    with_tags[i] = tuples_array[i][j:]\n",
    "\n",
    "    # DEBUGGING\n",
    "    # for i in range(0, len(with_tags)):\n",
    "    #  if with_tags[i][0][0].lower() not in q_words:\n",
    "    #      print(with_tags[i])\n",
    "\n",
    "    return with_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The preceding cells should be run first before running this cell!\n",
    "a = prune(tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sampling_data(sentence_list, category, dataset):\n",
    "    \"\"\"\n",
    "    Return the set of data specfied. The sentences in the sentences list is assumed to contain only thet\n",
    "    independents clause.\n",
    "\n",
    "    sentence_list -- a list that contains sentences which is represented as an array of tuples (word and pos).\n",
    "    category -- the label of the sentence\n",
    "    dataset -- the type of data needed:\n",
    "                1 - training set\n",
    "                2 - testing set\n",
    "                3 - all\n",
    "    \"\"\"\n",
    "\n",
    "    array = []\n",
    "    \n",
    "    # Training set\n",
    "    if dataset == 1:\n",
    "        y = int(math.ceil(len(sentence_list)*0.8))\n",
    "\n",
    "        for i in range(0, y):\n",
    "            data = []\n",
    "\n",
    "            for j in range(0, len(sentence_list[i])):\n",
    "                if j == 0:\n",
    "                    data.append(sentence_list[i][j][0].lower())\n",
    "                else:\n",
    "                    data.append(sentence_list[i][j][1].lower())\n",
    "\n",
    "            data.append(category[i].lower())\n",
    "            array.append(data)\n",
    "    # Testing set\n",
    "    elif dataset == 2:\n",
    "        x = int(len(sentence_list) - math.floor(len(sentence_list)*0.2))\n",
    "\n",
    "        for i in range(x, len(sentence_list)):\n",
    "            data = []\n",
    "\n",
    "            for j in range(0, len(sentence_list[i])):\n",
    "                if j == 0:\n",
    "                    data.append(sentence_list[i][j][0].lower())\n",
    "                else:\n",
    "                    data.append(sentence_list[i][j][1].lower())\n",
    "\n",
    "            data.append(category[i].lower())\n",
    "            array.append(data)\n",
    "    # All the data\n",
    "    elif dataset == 3:\n",
    "        for i in range(0, len(sentence_list)):\n",
    "            data = []\n",
    "\n",
    "            for j in range(0, len(sentence_list[i])):\n",
    "                if j == 0:\n",
    "                    data.append(sentence_list[i][j][0].lower())\n",
    "                else:\n",
    "                    data.append(sentence_list[i][j][1].lower())\n",
    "\n",
    "            data.append(category[i].lower())\n",
    "            array.append(data)\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format(sampling_type):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing data.\n",
    "    \n",
    "    NOTE: category[i] should be the label of sentence[i] in the labelled_data.csv file. \n",
    "    \"\"\"\n",
    "    \n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    \n",
    "    input1 = open(os.path.abspath('../files/pos_tagger.out'))\n",
    "    input2 = open(os.path.abspath('../files/raw_data/labelled_data.csv'))\n",
    "    input3 = open(os.path.abspath('../files/raw_data/labelled_data.csv'))\n",
    "\n",
    "    # Extract the word and tag pairs from the pos tagging output file\n",
    "    tuples = get_tags(input1)\n",
    "    # Get all question sentences in the file\n",
    "    sentences = get_raw_data(input2, 0)\n",
    "    # Get all the categories in the file\n",
    "    category = get_raw_data(input3, 1)\n",
    "\n",
    "    input1.close()\n",
    "    input2.close()\n",
    "    input3.close()\n",
    "\n",
    "    pruned_array = prune(tuples)\n",
    "    \n",
    "    training = []\n",
    "    testing = []\n",
    "    \n",
    "    start = 0\n",
    "    \n",
    "    if sampling_type == 'random':\n",
    "        training = get_sampling_data(pruned_array, category, 1)\n",
    "        testing = get_sampling_data(pruned_array, category, 2)\n",
    "        \n",
    "\n",
    "        return training, testing\n",
    "\n",
    "    elif sampling_type == 'stratified':\n",
    "        category_names = ['abbreviation', 'description', 'entity', 'human', 'location', 'numeric']\n",
    "\n",
    "        for label in category_names:\n",
    "            # get all sentence with the same label\n",
    "            for index in range(start, len(category)):\n",
    "                if category[index].lower() != label or index == len(category)-1:\n",
    "                    strata_sentence_list = pruned_array[start:index-1]\n",
    "                    strata_category = category[start:index-1]\n",
    "                    strata_training_set = get_sampling_data(strata_sentence_list, strata_category, 1)\n",
    "                    strata_testing_set = get_sampling_data(strata_sentence_list, strata_category, 2)\n",
    "                    training.extend(strata_training_set)\n",
    "                    testing.extend(strata_testing_set)\n",
    "                    start = index\n",
    "                    break\n",
    "\n",
    "        return training, testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, testing = format('stratified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method **write_to_file()** accepts unspecified number of arguments. It treats each argument as a dictionary key-value pair with *variable* as key and its *assigned value* as the value. The *kwargs keys* are the headers used in the csv output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_file(**kwargs):\n",
    "    path = os.path.abspath('../files/transformed_data.csv')\n",
    "\n",
    "    field_names = kwargs.keys()\n",
    "    data_frame = pandas.DataFrame()\n",
    "\n",
    "    for field in field_names:\n",
    "        value = kwargs.get(field)\n",
    "        string_array = []\n",
    "\n",
    "        for v in value:\n",
    "            s = ','.join(v)\n",
    "            string_array.append(s)\n",
    "\n",
    "        data_frame = pandas.concat([data_frame, pandas.DataFrame(\n",
    "            data=string_array, columns=[field])], axis=1)\n",
    "        \n",
    "    data_frame.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writes the file training and testing data into a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(training=training, testing=testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
